{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`DSML_WS_11` - Decision Trees**\n",
    "\n",
    "In this workshop we take a deep-dive on tree-based methods (and ensembles thereof) commonly used in a myriad of classification and regression problems.\n",
    "\n",
    "We will cover the following: \n",
    "1. **Task**: Classifying breast cancer samples\n",
    "1. **Task: Decision Trees for regression:** Predicting peak electricity demand\n",
    "1. **Decision Trees for classification**: Classifying breast cancer cells\n",
    "1. **Ensemble methods**: XGBoost, random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Task: Classifying breast cancer samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In ``Workshop 9``, we looked at the workings of relevant classification algorithms. One issue with we did not consider was that we have trained our algorithms on the full set of available data. While this is fine for understanding how classification works in general, it is not suitable for developing predictive models (as you know by now).\n",
    "\n",
    "As a result, the classification metrics from last week's workshop are relatively meaningless as we need to evaluate on previously unseen data. \n",
    "\n",
    "**Design a proper model development routine to train a high-performing classification algorithm for the breast cancer dataset. Proceed as follows:**\n",
    "\n",
    "- Define your feature (let's continue to focus on `area_mean` and `concave points_mean`) and target sets.\n",
    "- Partition the data into training, validation and test set, and scale the input features.\n",
    "- Train a support vector machine on the training set.\n",
    "- Tweak hyperparameters (e.g., kernel) by validating on the validation set.\n",
    "- Report test metrics from the unseen test set (only look at the test set once you are finished validating your model. Do not go back and forth as this would create leakage!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "cancer_df = pd.read_csv(\"breast_cancer.csv\", index_col = \"id\")\n",
    "\n",
    "# define feature and target\n",
    "x = cancer_df[['area_mean','concave points_mean']].values\n",
    "y = cancer_df['diagnosis'].values\n",
    "\n",
    "# partition data: 50% Train, 20% Validation, 30% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=42)\n",
    "X_train, X_hold, y_train, y_hold = train_test_split(X_train, y_train, test_size=(0.2/0.7),random_state=42)\n",
    "\n",
    "# scale input features (based on X_train)\n",
    "norm = StandardScaler()\n",
    "X_train_norm = norm.fit_transform(X_train)      # 284 Observations\n",
    "X_hold_norm = norm.transform(X_hold)            # 114 Observations\n",
    "X_test_norm = norm.transform(X_test)            # 171 Observations\n",
    "\n",
    "# For rounding digits in test metrics\n",
    "Digits = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate three `support vector machine` - model specifications that minimize $\\ell_{hinge}$-loss with different kernels:\n",
    "- $SVM_{linear}$: Linear hypothesis function $h_{\\theta}(x)$\n",
    "- $SVM_{poly}$: Linear hypothesis function $h_{\\theta}(x)$ with polynomial features\n",
    "- $SVM_{RBF}$: Linear hypothesis function $h_{\\theta}(x)$ with Radial Basis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training set\n",
    "model_linear = SVC(kernel='linear', coef0=1.0)\n",
    "model_linear.fit(X_train_norm, y_train)\n",
    "\n",
    "# evaluate on holdout set\n",
    "holdout_predictions = model_linear.predict(X_hold_norm)\n",
    "print(\"Holdout Accuracy =\", round(accuracy_score(y_hold, holdout_predictions),Digits))\n",
    "print(\"Holdout Precision =\", round(precision_score(y_hold, holdout_predictions, pos_label=\"M\"),Digits))\n",
    "print(\"Holdout Recall =\", round(recall_score(y_hold, holdout_predictions, pos_label=\"M\"),Digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training set\n",
    "model_poly = SVC(kernel='poly', C = 100, degree=2, coef0=1.0)   # Linear Hypothesis with Polynomial Features\n",
    "model_poly.fit(X_train_norm, y_train)\n",
    "\n",
    "# evaluate on holdout set\n",
    "holdout_predictions = model_poly.predict(X_hold_norm)\n",
    "print(\"Holdout Accuracy =\", round(accuracy_score(y_hold, holdout_predictions),Digits))\n",
    "print(\"Holdout Precision =\", round(precision_score(y_hold, holdout_predictions, pos_label=\"M\"),Digits))\n",
    "print(\"Holdout Recall =\", round(recall_score(y_hold, holdout_predictions, pos_label=\"M\"),Digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training set\n",
    "model_RBF = SVC(kernel='rbf', C = 100, coef0=1.0)           # Linear Hypothesis with RBF Features\n",
    "model_RBF.fit(X_train_norm, y_train)\n",
    "\n",
    "# evaluate on holdout set\n",
    "holdout_predictions = model_RBF.predict(X_hold_norm)\n",
    "print(\"Holdout Accuracy =\", round(accuracy_score(y_hold, holdout_predictions),Digits))\n",
    "print(\"Holdout Precision =\", round(precision_score(y_hold, holdout_predictions, pos_label=\"M\"),Digits))\n",
    "print(\"Holdout Recall =\", round(recall_score(y_hold, holdout_predictions, pos_label=\"M\"),Digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the classification metrics shows that:\n",
    "- Accuracy: $SVM_{RBF}$ < $SVM_{linear}$ < $SVM_{poly}$\n",
    "- Precision: $SVM_{RBF}$ < $SVM_{linear}$ < $SVM_{poly}$\n",
    "- Recall: $SVM_{RBF}$ < $SVM_{linear}$ $\\leq$ $SVM_{poly}$\n",
    "\n",
    "Hence, we choose $SVM_{poly}$ and report the final test metrics by evaluation of the test-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate preferred model (SVM with polynomial kernel, d=2, C=100) on test set\n",
    "test_predictions = model_poly.predict(X_test_norm)\n",
    "print(\"Test Accuracy =\", round(accuracy_score(y_test, test_predictions),Digits))\n",
    "print(\"Test Precision =\", round(precision_score(y_test, test_predictions, pos_label=\"M\"),Digits))\n",
    "print(\"Test Recall =\", round(recall_score(y_test, test_predictions, pos_label=\"M\"),Digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Task: Decision Trees for regression: Predicting peak electricity demand**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the tree-based regression introduced in ``Workshop 8``. In that session, we used a tree-based model to predict peak electricity demand based on temperature, splitting the data into training and testing sets. We also identified the optimal tree depth using a simple grid search over various tree depths. \n",
    "\n",
    "Instead of solely plotting the predicted regression line across all data in a scatterplot, we can also visually illustrate the model's desicion process using a tree diagram. This can be easily achieved with the ``plot_tree`` function from the ``sklearn.tree`` library.\n",
    "Let's look at a very simple tree architecture for the prediciton of peak electricity demand - to do this, proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Load the ``Pittsburgh_load_data.csv`` dataset and specify the input vectors for your model:\n",
    "- *Peak electricity demand* (``MAX``) as the prediction target\n",
    "- *High temperature* (``High_temp``) as the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "\n",
    "xp = df[\"High_temp\"]\n",
    "yp = df[\"MAX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Use the ``DesicisonTreeRegressor`` to fit a decision tree of max_depth = 2 to your data and plot the results in a scatterplot. Name the model *tree_model*.\n",
    "\n",
    "***Note:** For demonstration purposes, skip the training, validation and test splits, that would normally be required for prediction (as done in the first task). Our goal in this introductory setting is simply to get started with the visualization of tree diagrams and their interpretation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "Tree_reg = DecisionTreeRegressor(max_depth=2,\n",
    "                                 criterion=\"squared_error\")\n",
    "\n",
    "# train\n",
    "tree_model = Tree_reg.fit(xp.values.reshape((-1,1)), yp)\n",
    "\n",
    "# Further information on the fitted tree\n",
    "attributes = tree_model.tree_\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(xp, yp, marker=\"x\")\n",
    "plt.plot(np.arange(-18,40,1), Tree_reg.predict(np.arange(-18,40,1).reshape((-1,1))), marker=\"x\", color='C1')\n",
    "plt.xlabel(\"High Temperature (°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of nodes: \", attributes.node_count)\n",
    "print(\"Number of leafs: \", attributes.n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Visualize the tree diagram, by utilizing the ``plot_tree`` function.\n",
    "\n",
    "Start by initializing a figure with ``plt.figsize = (12,6)``, then use ``plot_tree(tree_model, feature_names=['High Temperature'])``.\n",
    "- Can you interpret the values that are depicted in the graphical representation of the tree?\n",
    "- How would you traverse the tree to predict peak electricity demand of a day with a high temperature of 30 C°?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plot_tree(tree_model, feature_names=['High Temperature'])\n",
    "plt.show()\n",
    "\n",
    "print(\"A temperature of 30 C° would lead to peak electricity demand of approx.\", round(tree_model.predict([[30]])[0],2), \"GW.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Decision Trees for classification: Classifying breast cancer cells**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "cancer_df = pd.read_csv(\"breast_cancer.csv\", index_col=\"id\")\n",
    "cancer_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To abstract from the relatively high-dimensionality of the breast cancer dataset let us confine our analysis to a two-dimensional feature vector consisting of `area_mean` and `concave points_mean` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for easier plotting\n",
    "def plot_cells():\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')\n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "    plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "    plt.legend(['Malignant','Benign'],fontsize=12)\n",
    "    \n",
    "plot_cells()\n",
    "plt.show()\n",
    "\n",
    "# Confine the dataset to the features 'area_mean' and 'concave points_mean'\n",
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# recode Y to 0 and 1\n",
    "Y  = np.where(Y==\"M\", int(1), Y) # Malign = 1\n",
    "Y  = np.where(Y==\"B\", int(0), Y) # Benign = 0\n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying the DecisionTreeClassifier and accessing its components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify and fit a simple `DecisionTreeClassifier`, which is available via `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(max_depth=2,criterion='gini') # we set gini as our impurity measure\n",
    "tree_classifier.fit(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision estimator has an attribute called tree_  which stores the entire tree structure and allows access to low-level attributes. The binary tree_ attribute is represented as a number of parallel arrays. The i-th element of each array holds information about the node `i`. Node 0 is the tree's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = tree_classifier.tree_         # All information to draw the tree is available through the tree_ function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Recall the basic terminology for decision trees, including nodes, leaves, parent and children nodes, threshold and impurity: What do they represent?\n",
    "</div>\n",
    "\n",
    "Among those arrays, we have:\n",
    "\n",
    "   - ``left_child``: ID of the left child of the node\n",
    "   - ``right_child``: ID of the right child of the node\n",
    "   - ``feature``: Feature used for splitting the node\n",
    "   - ``threshold``: Threshold value used for splitting the node\n",
    "   - ``impurity``: The impurity at the node\n",
    "   - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign various tree attributes\n",
    "n_nodes = structure.node_count\n",
    "n_leaves = structure.n_leaves\n",
    "children_left = structure.children_left\n",
    "children_right = structure.children_right\n",
    "feature = structure.feature\n",
    "threshold = structure.threshold\n",
    "impurity = structure.impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num nodes: \\t\",n_nodes)        # How many nodes does the tree have?\n",
    "print(\"Num leaves: \\t\",n_leaves)      # How many leaves does the tree have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Based on this information, can you draw the basic structure of the tree?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"left children per node: \", children_left)      # The seven values here correspond to the seven nodes above.\n",
    "print(\"right children per node: \", children_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Based on this information, can you add the node IDs to your basic tree sketch?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision feature at node: \", feature)          # Features area mean (0) and concave points (1) have been encoded to 0 and 1. -2 descibes a leaf\n",
    "print(\"Threshold of feature at node\", threshold)\n",
    "print(\"Impurity at node: \", impurity)                 # Impurity decreases as we run down the tree: Leaf nodes are more homogeneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Based on this information, can you derive which feature and threshold was used to split each node?\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieval of decision paths: An Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decision_path` method allows us to retrieve the node indicator functions. A non-zero element of an indicator matrix at the position (i, j) indicates that the sample i goes through the node j.\n",
    "\n",
    "- ``.indices`` stores all nodes that are visited by the sample [sample_id]\n",
    "- ``.indptr`` tells us, where the start [sample_id]- and end-nodes [sample_id+1] are in the .indices array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision_path Results in a 569x7 sparse matrix: n_samples x n_nodes\n",
    "node_indicators = tree_classifier.decision_path(X)     # Each row shows, which nodes were visited by the sample\n",
    "\n",
    "# let's generate a random sample ID                    Note: sample ID is a specific combination of features in our feature matrix\n",
    "sample_id = 10\n",
    "\n",
    "# retrieve decision_path for that sample:\n",
    "node_index = node_indicators.indices[node_indicators.indptr[sample_id] : node_indicators.indptr[sample_id + 1]]  #indptr maps the elements of data and indices to the rows of the sparse matrix\n",
    "\n",
    "print(\"Value of Feature ``Area Mean`` for Sample ID\", sample_id ,\"in Array:\", X[sample_id,0])\n",
    "print(\"Value of Feature ``Concave Points`` for Sample ID\", sample_id ,\"in Array:\", X[sample_id,1])\n",
    "print(\"Decision path for sample \" + str(sample_id), \": \", str(node_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> For our basic decision tree, what combinations of IDs could appear as decision paths?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method can be used to get the index of the leaf (Terminal Node) that each sample is predicted as from the incorporated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also retrieve the leaf ids reached by each sample using .apply\n",
    "leaf_ids = tree_classifier.apply(X)\n",
    "\n",
    "leaf_ids[:10]           # i.e. the terminal (leaf) node: Predicition, which our Sample IDs \"end up in\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> What IDs could be returned by apply()?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us also consider the features and thresholds that were used to predict a certain sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision path for sample %s: %s' % (str(sample_id), str(node_index)))\n",
    "print('Feature values of sample %s: %s \\n' % (sample_id, X[sample_id]))\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:                              # Recall: node_index stores nodes that are traversed by sample_id\n",
    "    # skip leaf node\n",
    "    if leaf_ids[sample_id] == node_id:\n",
    "        continue\n",
    "    \n",
    "    # for all other nodes, retrieve the feature values\n",
    "    if (X[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"Decision at node %s: value for feature %s (%s) is %s the threshold of %s\"\n",
    "          % (node_id,\n",
    "             feature[node_id],\n",
    "             X[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting the full decision tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "tree.plot_tree(tree_classifier,\n",
    "               class_names=['Malignant','Benign'],\n",
    "               feature_names=['area_mean', 'concave points_mean'])   # Malignant = 1, Benign = 0\n",
    "plt.show()   # Classification model: We use the majority vote to determine a nodes class. Default cutoff = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decision tree each node is represented by a box. For each node the following information is provided:\n",
    "- decision feature and threshold\n",
    "- impurity\n",
    "- number of samples\n",
    "- number of samples per class\n",
    "- class (i.e., majority vote)\n",
    "\n",
    "We can, thus, easily relate this back to the tree attributes we computed above. A selection is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num nodes: \\t\",n_nodes)\n",
    "print(\"Num leaves: \\t\",n_leaves)\n",
    "print(\"Feature at node\", feature) # -2 indicates no feature/threshold, i.e. a leaf\n",
    "print(\"Threshold of feature at node\", threshold)\n",
    "print(\"Impurity at node: \", impurity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting decision surfaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the lecture, another intuitive representation of decision trees is the use of decision surfaces. These can be related back directly to the decision tree. For ease of use, a plotting routine has been prepared that combines fitting and plotting into a single routine and allows for easy adjustment of tree depth and the minimum samples per leaf (discussed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_surface(max_depth,min_samples_leaf=1):    # The number inserted here determines the amount of leaves\n",
    "    \n",
    "    # specify and fit decision tree classifier\n",
    "    from sklearn.tree import DecisionTreeClassifier, export_graphviz # we also call the garphviz module for later visualization\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_leaf=min_samples_leaf,\n",
    "                                  criterion='gini') # we set entropy as our impurity measure\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    # get tree attributes\n",
    "    attributes = model.tree_\n",
    "    \n",
    "    # define range per feature\n",
    "    x_range = [0,2600] # i.e. mean area\n",
    "    y_range = [0, 0.21] # i.e mean conc. points\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # plot classification regions\n",
    "    grid=1000\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "\n",
    "    zz = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    zz = zz.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy,zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "    \n",
    "    # plot data points\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "    plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "    plt.legend([s1,s2],['Malignant','Benign'],fontsize=12)\n",
    "    \n",
    "    print(\"number of nodes: \", attributes.node_count)\n",
    "    print(\"number of leafs: \", attributes.n_leaves)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #plt.savefig(\"Breast_Cancer_Decision_Surface_{}depth.pdf\".format(tree_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(max_depth=2, min_samples_leaf=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Controlling overfitting in Decision Trees**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision-tree learners can create overly complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can easily be seen by increasing tree depth to unreasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(max_depth=15, min_samples_leaf=1)     # Tree reaches terminal state eventually \"is fully grown\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do about overfitting in sklearn? As mentioned, we have several tools at our disposal:\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. A too high value of maximum depth causes overfitting, whereas a too low value causes underfitting.\n",
    "- **min_samples_leaf**: By specifying a minimum number of samples per leaf, overfitting can be controlled for.\n",
    "- **ccp_alpha**: Cost Complexity (CCP) alpha paramter determining the size of the penalty."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Check the effect of the max_depth and the min_samples_leaf parameters. How would you use them to prevent overfitting?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(max_depth = 15, min_samples_leaf = 1)\n",
    "# increasing min_samples_leaf is decreasing tree size: Each Node needs to have a specific number of observations remaining\n",
    "# decreasing max_depth directly prunes the tree, counteracting overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cost Complexity of a Decision Tree: Effective Alphas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the **cost complexity** as an effective measure in avoiding overfitting. The cost complexity *CCP(T)* of a tree  is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "CCP(T) = ERR(T) + \\alpha \\cdot L(T)\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "- **ERR(T)** is the total misclassification rate of the terminal nodes\n",
    "- **L(T)** is the number of terminal nodes of tree T, also called *\"tree complexity\"*\n",
    "- **$\\alpha$** is a penalty parameter  related  to tree complexity L(T).\n",
    "\n",
    "This type of formula should look familiar, as it closely resembles the regularized regression loss functions we know.\n",
    "\n",
    "To get an idea of what values of $\\alpha$ could be appropriate, `scikit-learn` provides `DecisionTreeClassifier.cost_complexity_pruning_path`, that returns the **effective $\\alpha$ values** (i.e. the values of $\\alpha$, that will achieve the next step in complexity reduction) and the corresponding total leaf impurities at each step of the pruning process. As $\\alpha$ increases, more of the tree is pruned, which increases the total impurity of its leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "# fit decision tree (without limit on max_depth, i.e. tree will grow fully if alpha is set to 0)\n",
    "tree_classifier = DecisionTreeClassifier(random_state=0, \n",
    "                                         criterion=\"gini\")\n",
    "\n",
    "# compute cost_complexity_pruning_path \n",
    "path = tree_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "pd.DataFrame({\"Effective Alpha\": ccp_alphas,\n",
    "             \"Gini-Index\": impurities})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``ccp_alphas`` stores a list of alphas, which lead to the next pruning step. ``impurities`` gives out the values of the chosen homogeneity-measure.\n",
    "\n",
    "We can visualize this in a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cost_complexity_pruning_path\n",
    "fig, ax = plt.subplots(figsize=(8,6)) # plot cccp alphas against impurities\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")  # we remove the last alpha as this corresponds to a tree with only the root node\n",
    "ax.set_xlabel(\"effective alpha\",fontsize=16)\n",
    "ax.set_ylabel(\"total impurity of leaves\",fontsize=16)\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\",fontsize=16)\n",
    "plt.show()\n",
    "#plt.savefig(\"Determining_Alpha.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a decision tree using the effective alphas. The last value in ccp_alphas is the alpha value that prunes the whole tree, leaving the tree with one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    tree.fit(X_train, y_train)\n",
    "    trees.append(tree)\n",
    "print(\"Number of nodes in the last tree is {} with ccp_alpha: {}\".format(\n",
    "      trees[-4].tree_.node_count, ccp_alphas[-4]))     # If changed to -2, this is a two layer tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of this example, we remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\\\n",
    "Here we show that the number of nodes and tree depth decreases as alpha increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are showing the pruning effect on the tree\n",
    "trees = trees[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [tree.tree_.node_count for tree in trees]\n",
    "depth = [tree.tree_.max_depth for tree in trees]\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\",fontsize=16)\n",
    "ax[0].set_ylabel(\"number of nodes\",fontsize=16)\n",
    "ax[0].set_title(\"Number of nodes vs alpha\",fontsize=16)\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\",fontsize=16)\n",
    "ax[1].set_ylabel(\"depth of tree\",fontsize=16)\n",
    "ax[1].set_title(\"Depth vs alpha\",fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(\"Pruning_effect.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase alpha, we penalize more complex tree structures. Hence, the number of nodes and the depth of the tree decreases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Effective Alphas and Predictive Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can implement a grid search over the identified effective alphas to determine where predictive performance is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = []\n",
    "scores_val = []\n",
    "\n",
    "# run loop\n",
    "for ccp_alpha in ccp_alphas:\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_val = accuracy_score(y_test, model.predict(X_test))\n",
    "    scores_val.append(accuracy_val)\n",
    "    scores_train.append(accuracy_train)\n",
    "\n",
    "# collect results in DF\n",
    "df = pd.DataFrame(columns=[\"alpha\", \"train_score\", \"test_score\"])\n",
    "df[\"alpha\"] = ccp_alphas\n",
    "df[\"train_score\"] = scores_train\n",
    "df[\"test_score\"] = scores_val\n",
    "\n",
    "# show top five alphas\n",
    "df.sort_values(\"test_score\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (8,6))\n",
    "plt.plot(df[\"alpha\"], df[\"train_score\"], marker = 'o')\n",
    "plt.plot(df[\"alpha\"], df[\"test_score\"], marker = 'o')\n",
    "plt.legend([\"train\", \"test\"])\n",
    "plt.xlabel(\"effective alpha\", fontsize =  16)\n",
    "plt.ylabel(\"accuracy\", fontsize =  16)\n",
    "plt.title(\"Accuracy for different alphas\", fontsize = 16)\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(\"alpha_tuning.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the y-axis reflects accuracy, not error! The *best* model is hence achieved with $\\alpha \\approx 0.004$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **-- Small excursion: Naive Bayes --**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last week's lecture, you also learned about the Naive Bayes algorithm. It is based on conditional probabilities that you use to calculate the change in probability of class membership. Say, for example, you have an unknown cell structure and want to calculate how likely it is that this cell belongs to the Malignant cells. You can, then, look at the conditional probability of a cell having that specific mean area to predict whether the unknown cell is malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Naive Bayes from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()                     # Gaussian Naive Bayes assumes that each feature follows a Gaussian normal distribution indepently (!) for each class\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "point_benign, point_malignant = gnb.theta_              # gives the mean of each feature per class: The center of the gaussian\n",
    "radius_benign, radius_malignant = np.sqrt(gnb.var_)     # gives the SD of each feature per class: The spread around the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpl_patches\n",
    "benign = mpl_patches.Ellipse(xy=point_benign, width=3 * radius_benign[0], height=3 * radius_benign[1], facecolor='none', edgecolor='b', linewidth=0.5)       # Multiplication with 3 Standard Deviation includes 99.7% of the data, assuming gaussian distribution\n",
    "malignant = mpl_patches.Ellipse(xy=point_malignant, width=3 * radius_malignant[0], height=3 *radius_malignant[1], facecolor='none', edgecolor='r', linewidth=0.5)\n",
    "plt.scatter(point_benign[0], point_benign[1], color='b')\n",
    "plt.scatter(point_malignant[0], point_malignant[1], color='r')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(benign)\n",
    "ax.add_artist(malignant)\n",
    "\n",
    "s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3', linewidths=0.2)\n",
    "s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0', linewidths=0.2)    \n",
    "plt.xlim([0,2600])\n",
    "plt.ylim([0,0.21])\n",
    "plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "plt.legend([s1,s2],['Malignant','Benign'],fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could now evaluate and compare both models using the known cross-validation procedure and classification evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Ensemble Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In predictive modeling, “risk” is equivalent to variation (i.e. variance) in prediction error. Ensemble methods are targeted at reducing variance, thus increasing predictive power.\n",
    "The core idea is that by combining the outcomes of individual models, e.g., by taking an average, variance may be reduced. Thus, using an average of two or more predictions can potentially lead to smaller error variance, and therefore better predictive power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not discuss ensemble methods in detail here, but will limit our discussion to a brief introduction of two very popular tree-based ensemble methods. These are:\n",
    "\n",
    "- RandomForest: see [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "- XGBoost (a type a boosting method): see [here](https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bagging:** Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests is a selection of n trees which are trained in parallel. Predictions are made by averaging the outputs across these n trees. Random Forest are most often combined with **bagging** (**B**ootstrap **agg**regat**ing**), i.e. different boostrap samples of the training data are used to train the individual trees.\n",
    "- Each tree is trained on a different bootstrap sample of training data\n",
    "- At each node in a tree, it only looks at random subsets of features when choosing the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split on breast cancer dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sepcify and fit model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, \n",
    "                                       bootstrap=True, random_state=42) # we select boostrapp, i.e. we use bagging\n",
    "R_FOREST = rf_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(y_test,R_FOREST.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,R_FOREST.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by taking the default setting, we obtain very good results that are comparable to those of the fully grid-searched decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Boosting:** XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an ensemble method that uses **boosting**. While XGBoost is not included in sklearn, there is a very well developed API that can be installed by executing the following command:\n",
    "- `conda install -c conda-forge xgboost`\n",
    "\n",
    "Once you have completed the installation you are good to go. Let us fit a very simple classifier to the breast cancer dataset.\n",
    "\n",
    "XGBoost is a specific implementation of boosting that is known for being very fast and efficient. The trees are trained independently, where each tree is trained to fix the errors made by the previous trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify and fit model\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(booster=\"gbtree\")\n",
    "XGBOOST = xgb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(y_test,XGBOOST.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,XGBOOST.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there is likely room for improvement as you grid search some of the hyperparameters. However, by just taking the default setting, we already achieve an accuracy score that is comparable to that of the grid-searched decision tree above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
